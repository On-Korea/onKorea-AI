# -*- coding: utf-8 -*-
"""
ì¤‘êµ¬ì²­ ì™¸êµ­ì¸ ì§€ì› ì •ì±… í¬ë¡¤ëŸ¬ (ìë™ í‚¤ ë¶„ë¥˜ + ë³¸ë¬¸ë§Œ íŒŒì‹± + í†µì¼ ì¹¼ëŸ¼ ì €ì¥)
- ëª©ë¡ URLì—ì„œ ê²Œì‹œë¬¼ ìë™ ìˆ˜ì§‘
- ë³¸ë¬¸ì€ 'â–¶ ì œëª©' + 'ã…‡ í‚¤:ê°’' + ììœ  í˜•ì‹ ìë™ íŒŒì‹±
- í‚¤ì›Œë“œ í¬í•¨ ê¸°ë°˜ ìë™ ë¶„ë¥˜ (ëŒ€ìƒ, ê¸°ê°„, ë°©ë²•, ë¬¸ì˜, ì¥ì†Œ, ê¸ˆì•¡ ë“±)
- ì‹¤í–‰ ë•Œë§ˆë‹¤ ìƒˆ íŒŒì¼ë¡œ ì €ì¥ (CSV / JSON)
"""

import re
import json
import time
import pathlib
import pandas as pd
import requests
from bs4 import BeautifulSoup

# ===== ì§€ì—­/ìƒìˆ˜ =====
REGION = "ì¤‘êµ¬"
LIST_URL = ["https://www.junggu.seoul.kr/content.do?cmsid=16539",
            "https://www.junggu.seoul.kr/content.do?cmsid=16540",]
HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; DataCollectionBot/1.0)"}

COLS = [
    "region","source_category","item_title","target","period","content",
    "method","contact","location","full_text","source_url"
]

# ===== ê³µí†µ ì •ê·œì‹ =====
ZERO_WIDTH = re.compile(r"[\u200b\u200c\u200d\u2060\ufeff]")
PHONE_SEQ  = re.compile(r"(?:\d{2,4}\s*-\s*\d{3,4}\s*-\s*\d{4})(?:\s*/\s*(?:\d{2,4}\s*-\s*\d{3,4}\s*-\s*\d{4}))*")
KEY_VALUE_PAT = re.compile(r"\s*(.+?)\s*[:ï¼š\-â€“]\s*(.+)$")
ANGLE_TITLE_PAT = re.compile(r"^[\s]*[<ã€ˆã€Š](.+?)[>ã€‰ã€‹]\s*$")

# ========== ìë™ í‚¤ ë¶„ë¥˜ í•¨ìˆ˜ ==========
def normalize_key(k: str) -> str:
    """í•œê¸€ í‚¤ì›Œë“œ ë¹„êµìš©: íŠ¹ìˆ˜ë¬¸ì, ê³µë°±, ê´„í˜¸ ì œê±°"""
    k = ZERO_WIDTH.sub("", k)
    k = k.replace("\u00A0", " ").replace("\u3000", " ")
    k = re.sub(r"[â˜…â˜†â€¢Â·â–¶â–·â– â—‹â—â€»\-\=\+\(\)\[\]\{\}<>]", "", k)
    k = re.sub(r"\s+", "", k)
    # ìˆ«ìë‚˜ ì½œë¡ (:) ë’¤ì— ë¶™ì€ ë¬¸ìëŠ” ë²„ë¦¼
    k = re.sub(r"[:0-9].*$", "", k)
    return k.strip()



def auto_map_key(pre_norm: str) -> str:
    """í‚¤ ë¬¸ìì—´ ì•ˆì— í¬í•¨ëœ ë‹¨ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìë™ ë§¤í•‘."""
    if any(word in pre_norm for word in ["ëŒ€ìƒ", "ìê²©"]):
        return "target"
    if any(word in pre_norm for word in ["ê¸°ê°„", "ì¼ì‹œ", "ì‹œê°„", "ê¸°í•œ","ì¼ì •"]):
        return "period"
    if any(word in pre_norm for word in ["ë°©ë²•","ê¸ˆì•¡"]):
        return "method"
    if any(word in pre_norm for word in ["ë¬¸ì˜", "ì—°ë½ì²˜", "ì „í™”ë²ˆí˜¸", "ë‹´ë‹¹","ì ‘ìˆ˜"]):
        return "contact"
    if any(word in pre_norm for word in ["ì¥ì†Œ", "ìœ„ì¹˜", "ì§€ì—­", "ìš´ì˜ì¥ì†Œ","í˜„í™©"]):
        return "location"
    if any(word in pre_norm for word in ["ë‚´ìš©", "ì†Œê°œ", "ê°œìš”", "ì„¤ëª…", "ë³´ì¥ë‚´ìš©", "ì§€ì›ë‚´ìš©", "ì‚¬ì—…ë‚´ìš©"]):
        return "content"  # âœ… ìƒˆë¡œ ì¶”ê°€
    return None  # ì¸ì‹ ì•ˆ ë˜ë©´ contentë¡œ


# ========== ë³¸ë¬¸ ì •ë¦¬ í•¨ìˆ˜ ==========
def fetch_html(url: str) -> str:
    for i in range(3):
        r = requests.get(url, headers=HEADERS, timeout=15)
        if r.ok and r.text.strip():
            return r.text
        time.sleep(1 + i)
    r.raise_for_status()

def node_text_with_newlines(node: BeautifulSoup) -> str:
    """HTML ë³¸ë¬¸ â†’ ì¤„ë°”ê¿ˆ í¬í•¨ëœ í…ìŠ¤íŠ¸ ì •ë¦¬ (ìš´ì˜ê¸°ê°„/ìš´ì˜ëŒ€ìƒ ì¤„ êµ¬ë¶„ ê°•í™”)"""
    
    # <br>ì„ \nìœ¼ë¡œ ë³€í™˜
    for br in node.find_all("br"):
        br.replace_with("\n")

    # ë§í¬ ì²˜ë¦¬ (href ë‚¨ê¸°ê¸°)
    for a in node.find_all("a"):
        href = a.get("href", "").strip()
        text = a.get_text(" ", strip=True)
        if href:
            a.replace_with(f"{text} ({href})")
        else:
            a.replace_with(text)

    # âœ… ì œëª©(<strong>ì´ë‚˜ <b>)ì— â–¶ê°€ í¬í•¨ëœ ê²½ìš° ì¤„ë°”ê¿ˆ ì¶”ê°€
    for strong_tag in node.find_all(["strong", "b"]):
        text = strong_tag.get_text(" ", strip=True)
        # â–¶, â–·, â–º ë“±ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ìƒˆ ì¤„ ì•ë’¤ë¡œ ì¶”ê°€
        if re.match(r"^[â–¶â–·â–º]", text):
            strong_tag.replace_with(f"\n{text}\n")
        else:
            strong_tag.replace_with(text)

    # ì¸ë¼ì¸ íƒœê·¸ í‰íƒ„í™”
    for tag in node.find_all(["u", "em", "span"]):
        tag.replace_with(tag.get_text(" ", strip=True))

    # ë¸”ë¡ íƒœê·¸ ê°œí–‰ ì¶”ê°€
    for tag in node.find_all(["p", "li", "div"]):
        tag.replace_with(tag.get_text(" ", strip=True) + "\n")

    # ì „ì²´ í…ìŠ¤íŠ¸
    text = node.get_text("\n")
    text = ZERO_WIDTH.sub("", text)
    text = re.sub(r"&nbsp;?", " ", text)
    text = re.sub(r"[ \t]+", " ", text)

    # âœ… URL ì•ë’¤ë¡œ ê°œí–‰
    text = re.sub(r"(https?://[^\s]+)", r"\n\1\n", text)

    # âœ… ì¤‘êµ¬ì²­ ìŠ¤íƒ€ì¼ (â—‹, ã…‡, - ë“±)
    text = re.sub(r"(?=[ã…‡â—‹â—\-â€»â˜…]\s*[ê°€-í£])", "\n", text)

    # âœ… í•µì‹¬ ì¶”ê°€ â€” â–¶, â–·, â–º ë“± ì•ë’¤ë¡œ ê°•ì œ ê°œí–‰ ì¶”ê°€
    text = re.sub(r"([â–¶â–·â–º])", r"\n\1", text)

    # âœ… ì¤‘ë³µ ê°œí–‰ ì •ë¦¬
    text = re.sub(r"\n{2,}", "\n", text)
    return text.strip()




# ========== ëª©ë¡ ìˆ˜ì§‘ ==========
def collect_detail_urls_from_list(list_url: str, max_pages: int = 30, sleep_sec: float = 0.6):
    urls, seen = [], set()
    for page in range(1, max_pages + 1):
        list_page = f"{list_url}&page={page}"
        print(f"ğŸ“„ í˜ì´ì§€ {page} ìš”ì²­ ì¤‘: {list_page}")
        try:
            html = fetch_html(list_page)
        except Exception as e:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
            break

        soup = BeautifulSoup(html, "lxml")
        before = len(urls)
        for a in soup.select("a[href*='mode=view'][href*='cid=']"):
            href = a.get("href", "").strip()
            if not href:
                continue
            if not href.startswith("http"):
                href = requests.compat.urljoin(list_url, href)
            if href not in seen:
                seen.add(href)
                urls.append(href)
        if len(urls) == before:
            break
        time.sleep(sleep_sec)
    print(f"âœ… ì´ {len(urls)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    return urls


# ========== ë³¸ë¬¸ íŒŒì‹± ==========
def parse_detail_lines_to_fields(detail_lines):
    fields = {}
    current_section = None
    locked_section = None  # í˜„ì¬ í•„ë“œ ê³ ì • ì—¬ë¶€

    for raw in detail_lines:
        text = ZERO_WIDTH.sub("", raw).strip()
        if not text:
            continue
        text = re.sub(r"&nbsp;?", " ", text)

        # ğŸ”’ ì ê¸ˆ ìƒíƒœì¸ ê²½ìš°
        if locked_section:
            # ìƒˆ í‚¤ì›Œë“œ íƒì§€ (ex. ì ‘ìˆ˜, ë¬¸ì˜, ì¥ì†Œ ë“±)
            check_key = re.sub(r"^[\sâ—‹â—â€»\-â€“â€¢Â·âˆ™â‹…â˜†â–¶â–·]*", "", text)
            key_norm = normalize_key(check_key.split(":", 1)[0].split("-", 1)[0])
            new_mapped = auto_map_key(key_norm)

            # âœ… method ì™¸ ë‹¤ë¥¸ ëª…ì‹œì  í‚¤ ë“±ì¥ ì‹œì—ë§Œ í•´ì œ
            if new_mapped and new_mapped not in (None, locked_section):
                locked_section = None
                current_section = new_mapped
                # ê·¸ ì¤„ì€ ìƒˆ í•„ë“œë¡œ ì €ì¥
                val = ""
                if re.search(r"[:ï¼š\-â€“]", text):
                    parts = re.split(r"[:ï¼š\-â€“]", text, maxsplit=1)
                    val = parts[1].strip() if len(parts) > 1 else ""
                fields[new_mapped] = f"{fields.get(new_mapped, '')} / {val}".strip(" /")
                continue

            # ê·¸ ì™¸ì—ëŠ” ê·¸ëŒ€ë¡œ í˜„ì¬ ì„¹ì…˜(method)ì— ì´ì–´ë¶™ì´ê¸°
            fields[locked_section] = f"{fields.get(locked_section, '')} / {text}".strip(" /")
            continue

                # ğŸŸ¢ ì¼ë°˜ ì²˜ë¦¬ ì‹œì‘
        # ê¸°ì¡´ stripped = re.sub(...) â†’ ìˆ˜ì •
        if text.lstrip().startswith("â€»"):
            # â€» ë¬¸ì¥ì€ ì£¼ì„ì´ë‚˜ ì°¸ê³  ë¬¸ì¥ìœ¼ë¡œ ê°„ì£¼ â†’ ë‚´ìš© ìœ ì§€
            stripped = text.strip()
        else:
            stripped = re.sub(
                r"^[\sã€€]*(?:[â–¶â–·â—‹â—â–¡â– â—†â—‡â˜†\-â€“â€¢Â·âˆ™â‹…â—â—¦â])\s*(?=\(?[ê°€-í£A-Za-z0-9])",
                "",
                text
            ).strip()

        

        if re.search(r"[:ï¼š]\s*", stripped):
            key, val = re.split(r"[:ï¼š]", stripped, maxsplit=1)
            key_norm = normalize_key(key)
            mapped = auto_map_key(key_norm)
            if mapped:
                # ê¸°ì¡´
                # fields[mapped] = f"{fields.get(mapped, '')} / {val.strip()}".strip(" /")
                # ë³€ê²½ âœ…
                fields[mapped] = f"{fields.get(mapped, '')} / {key.strip()} : {val.strip()}".strip(" /")
                current_section = mapped
                if mapped == "method":
                    locked_section = "method"
                continue


        # (2) "í‚¤ - ê°’"
        m_dash = re.match(r"^([ê°€-í£\s]+?)\s*[-â€“]\s*(.+)$", stripped)
        if m_dash:
            key, val = m_dash.groups()
            key_norm = normalize_key(key)
            mapped = auto_map_key(key_norm)
            if mapped:
                # ê¸°ì¡´
                # fields[mapped] = f"{fields.get(mapped, '')} / {val.strip()}".strip(" /")
                # ë³€ê²½ âœ…
                fields[mapped] = f"{fields.get(mapped, '')} / {key.strip()} - {val.strip()}".strip(" /")
                current_section = mapped
                if mapped == "method":
                    locked_section = "method"
                continue


        # (3) ë‹¨ë… í‚¤ì›Œë“œ ì¤„ (â€˜ì´ìš©ë°©ë²•â€™, â€˜ì‘ëª¨ë°©ë²•â€™ ë“±)
        key_norm = normalize_key(stripped)
        mapped = auto_map_key(key_norm)
        if mapped:
            current_section = mapped
            if mapped == "method":
                locked_section = "method"
            continue

        # (4) ì¼ë°˜ ë¬¸ì¥ â€” í˜„ì¬ ì„¹ì…˜ ìœ ì§€
        target = current_section or "content"
        fields[target] = f"{fields.get(target, '')} / {text}".strip(" /")

    return fields









# ========== ì•„ì´í…œ ë‹¨ìœ„ ë¶„ë¦¬ ==========
def split_items(text: str, fallback_title: str = None):
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    items, current_title, current_details = [], None, []
    drop_tokens = ("ì²¨ë¶€", "ë°”ë¡œë³´ê¸°", "ëª©ë¡", "ì´ì „ê¸€", "ë‹¤ìŒê¸€")

    def flush_if_has_details():
        nonlocal items, current_title, current_details
        if current_title and current_details:
            items.append({
                "item_title": current_title.strip(),
                "detail_lines": current_details
            })
        current_title, current_details = None, []

    for ln in lines:
        if ln.startswith(drop_tokens):
            continue

        # âœ… "â–¶" ë˜ëŠ” "ã€ˆì œëª©ã€‰" íŒ¨í„´ ë“±ì¥ ì‹œ ìƒˆ itemìœ¼ë¡œ ë¶„ë¦¬
        if ln.startswith("â–¶") or re.match(r"^[<ã€ˆã€Š].+[>ã€‰ã€‹]$", ln) or re.match(r"^\d+\.\s*[A-Za-zê°€-í£]", ln):
            flush_if_has_details()
            current_title = re.sub(r"^[â–¶\s]*", "", ln).strip()
            continue

        # âœ… ë³¸ë¬¸ ì²« ì¤„ì´ fallback ì œëª©ê³¼ ê°™ìœ¼ë©´ ìƒëµ
        if fallback_title:
            clean_ln = re.sub(r"[<ã€ˆã€Š>ã€‰ã€‹]", "", ln).strip()
            clean_title = re.sub(r"[<ã€ˆã€Š>ã€‰ã€‹]", "", fallback_title).strip()
            if clean_ln == clean_title:
                continue

        # âœ… ì¼ë°˜ ë³¸ë¬¸ ì¤„
        # ê¸€ë¨¸ë¦¬í‘œ(ã…‡, â—‹, - ë“±)ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì„œ parse_detail_lines_to_fields()ë¡œ ë„˜ê¹€
        if current_title:
            current_details.append(ln)

    # ë§ˆì§€ë§‰ í•­ëª© ì¶”ê°€
    flush_if_has_details()

    # âœ… ë§Œì•½ â–¶ êµ¬ë¶„ì´ ì—†ìœ¼ë©´ fallbackìœ¼ë¡œ ì „ì²´ë¥¼ í•˜ë‚˜ë¡œ ë¬¶ìŒ
    if not items and lines:
        items = [{
            "item_title": fallback_title or "(ë³¸ë¬¸)",
            "detail_lines": lines
        }]

    return items




# ========== 1ê±´ íŒŒì‹± ==========
def build_records(url: str):
    html = fetch_html(url)
    soup = BeautifulSoup(html, "lxml")

    # ì œëª© íƒìƒ‰
    title_el = soup.select_one("th.view_tit, div.board_view_02 h3, div.board_view h3")
    item_title = title_el.get_text(strip=True) if title_el else ""

    # ë³¸ë¬¸ ë…¸ë“œ
    node = soup.select_one("div.board_view_02 td.view_txt, td.view_txt, div.view_txt, .board_view, article")
    if not node:
        node = soup

    text = node_text_with_newlines(node)
    items = split_items(text, fallback_title=item_title)
    records = []

    for it in items:
        fields = parse_detail_lines_to_fields(it["detail_lines"])
        rec = {
            "region": REGION,
            "source_category": "",
            "item_title": it["item_title"],
            "target": fields.get("target", ""),
            "period": fields.get("period", ""),
            "content": fields.get("content", ""),
            "method": fields.get("method", ""),
            "contact": fields.get("contact", ""),
            "location": fields.get("location", ""),
            "full_text": "â–¶ " + it["item_title"] + "\n" + "\n".join(it["detail_lines"]),
            "source_url": url
        }
        records.append(rec)
    return records


# ========== ì €ì¥ ==========
def save_records(records, outdir="out", base_name="ì¤‘êµ¬"):
    out = pathlib.Path(outdir)
    out.mkdir(parents=True, exist_ok=True)

    df = pd.DataFrame(records)
    for c in COLS:
        if c not in df.columns:
            df[c] = ""
    df = df[COLS]
    df.to_csv(out / f"{base_name}.csv", index=False, encoding="utf-8-sig")
    df.to_json(out / f"{base_name}.json", orient="records", force_ascii=False, indent=2)
    print(f"âœ… ì €ì¥ ì™„ë£Œ ({len(df)}ê±´)\n{out / f'{base_name}.csv'}")

# ========== ì‹¤í–‰ ==========
if __name__ == "__main__":
    all_records = []

    for list_url in LIST_URL:  # âœ… ì—¬ëŸ¬ ëª©ë¡ URL ìˆœíšŒ
        print(f"ğŸŒ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘: {list_url}")
        urls = collect_detail_urls_from_list(list_url, max_pages=30)
        for u in urls:
            try:
                all_records.extend(build_records(u))
            except Exception as e:
                print(f"âš ï¸ {u} ì˜¤ë¥˜: {e}")

    save_records(all_records)
