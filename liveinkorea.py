# -*- coding: utf-8 -*-
"""
ì„œìš¸ ê° êµ¬ ê°€ì¡±ì„¼í„° (liveinkorea.kr) í”„ë¡œê·¸ë¨ í¬ë¡¤ëŸ¬
- ìë™ í‚¤ ë¶„ë¥˜ + ë³¸ë¬¸ íŒŒì‹± + ì´ë¯¸ì§€ URL ìˆ˜ì§‘
- CSV / JSON ìë™ ì €ì¥
"""

import re
import json
import time
import pathlib
import pandas as pd
import requests
from bs4 import BeautifulSoup

# ===== ê¸°ë³¸ ìƒìˆ˜ =====
BASE_URL = "https://liveinkorea.kr/web/lay1/bbs/S1T10C27/A/4/list.do"
HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; DataCollectionBot/1.0)"}

# âœ… ì§€ì—­ ì£¼ì†Œì½”ë“œ (Dì–´ì©Œê³ )
SEOUL_DISTRICTS = {  "ì¢…ë¡œêµ¬": "D001", "ì¤‘êµ¬": "D002", "ìš©ì‚°êµ¬": "D003", "ì„±ë™êµ¬": "D004", "ê´‘ì§„êµ¬": "D005",
    "ë™ëŒ€ë¬¸êµ¬": "D006", "ì¤‘ë‘êµ¬": "D007", "ì„±ë¶êµ¬": "D008", "ê°•ë¶êµ¬": "D009", "ë„ë´‰êµ¬": "D010",
    "ë…¸ì›êµ¬": "D011", "ì€í‰êµ¬": "D012", "ì„œëŒ€ë¬¸êµ¬": "D013", "ë§ˆí¬êµ¬": "D014", "ì–‘ì²œêµ¬": "D015",
    "ê°•ì„œêµ¬": "D016", "êµ¬ë¡œêµ¬": "D017", "ê¸ˆì²œêµ¬": "D018", "ì˜ë“±í¬êµ¬": "D019", "ë™ì‘êµ¬": "D020",
    "ê´€ì•…êµ¬": "D021", "ì„œì´ˆêµ¬": "D022", "ê°•ë‚¨êµ¬": "D024", "ì†¡íŒŒêµ¬": "D025","ê°•ë™êµ¬": "D026"
}

#ê³ ì • ì¹¼ëŸ¼
COLS = [
    "region","source_category","item_title","target","period","content",
    "method","contact","location","image","full_text","source_url"
]

# ===== ê³µí†µ ì •ê·œì‹ =====
ZERO_WIDTH = re.compile(r"[\u200b\u200c\u200d\u2060\ufeff]")

# ========== ìë™ í‚¤ ë¶„ë¥˜ í•¨ìˆ˜ ==========
def normalize_key(k: str) -> str:
    k = ZERO_WIDTH.sub("", k)
    k = k.replace("\u00A0", " ").replace("\u3000", " ")
    k = re.sub(r"[â˜…â˜†â€¢Â·â–¶â–·â– â—‹â—â€»\-\=\+\(\)\[\]\{\}<>]", "", k)
    k = re.sub(r"\s+", "", k)
    k = re.sub(r"[:0-9].*$", "", k)
    return k.strip()

#===== í‚¤ì›Œë“œë¡œ ì¹¼ëŸ¼ ë¶„ë¥˜ ======
def auto_map_key(pre_norm: str) -> str:
    if any(word in pre_norm for word in ["ëŒ€ìƒ", "ìê²©"]):
        return "target"
    if any(word in pre_norm for word in ["ê¸°ê°„", "ì¼ì‹œ", "ì‹œê°„", "ê¸°í•œ","ì¼ì •"]):
        return "period"
    if any(word in pre_norm for word in ["ë°©ë²•","ê¸ˆì•¡","ì‹ ì²­","ì ‘ìˆ˜"]):
        return "method"
    if any(word in pre_norm for word in ["ë¬¸ì˜", "ì—°ë½ì²˜", "ì „í™”ë²ˆí˜¸", "ë‹´ë‹¹"]):
        return "contact"
    if any(word in pre_norm for word in ["ì¥ì†Œ", "ìœ„ì¹˜", "ì§€ì—­", "ìš´ì˜ì¥ì†Œ"]):
        return "location"
    if any(word in pre_norm for word in ["ë‚´ìš©","ì†Œê°œ","ê°œìš”","ì„¤ëª…","ì§€ì›ë‚´ìš©"]):
        return "content"
    return None

# ========== HTML ìš”ì²­ ==========
def fetch_html(url: str) -> str:
    for i in range(3):
        r = requests.get(url, headers=HEADERS, timeout=15)
        if r.ok and r.text.strip():
            return r.text
        time.sleep(1 + i)
    r.raise_for_status()



# ========== ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì •ë¦¬ ==========
def node_text_with_newlines(node: BeautifulSoup) -> tuple[str, str]:
    image_urls = []
    for img in node.find_all("img"):
        src = img.get("src", "")
        if not src:
            continue
        src = src.strip()

        # âœ… ì ˆëŒ€ê²½ë¡œê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë¶™ì´ê¸°
        if not re.match(r"^https?://", src):
            src = requests.compat.urljoin("https://liveinkorea.kr/", src)

        image_urls.append(src)
        img.decompose()  # ì´ë¯¸ì§€ íƒœê·¸ ì œê±°

    for p in node.find_all("p"):
        if "<ul" in p.text or "<li" in p.text or "<span" in p.text:
            try:
                inner_soup = BeautifulSoup(p.text, "lxml")
                p.clear()
                p.append(inner_soup)
            except Exception:
                pass

    for tag in node.find_all(True):
        for attr in list(tag.attrs):
            if attr.startswith("data-") or attr in ("style", "class", "id", "align"):
                del tag[attr]

    for tag in node.find_all(["span", "font", "b", "i", "u", "strong", "em"]):
        tag.unwrap()

    for li in node.find_all("li"):
        li.replace_with(li.get_text(strip=True) + "\n")
    for ul in node.find_all("ul"):
        ul.replace_with(ul.get_text("\n", strip=True) + "\n")
    for br in node.find_all("br"):
        br.replace_with("\n")
    for p in node.find_all("p"):
        p.insert_before("\n")
        p.insert_after("\n")


    text = node.get_text("\n", strip=True)
    text = re.sub(r"\n{2,}", "\n", text)
    text = re.sub(r"\s+$", "", text)
    text = re.sub(r"(<[^>]+>)", "", text)
    text = text.strip()

    
    return text, "; ".join(image_urls)




# ========== ëª©ë¡ ìˆ˜ì§‘ ==========
def collect_detail_urls(region: str, code: str, max_pages: int = 5):
    urls, seen = [], set()
    for page in range(1, max_pages + 1):
        list_page = f"{BASE_URL}?search_recruit_stat=02&area=A001&area_detail={code}&cpage={page}"
        print(f"ğŸ“„ [{region}] í˜ì´ì§€ {page} ìš”ì²­ ì¤‘: {list_page}")
        try:
            html = fetch_html(list_page)
        except Exception as e:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
            break
        soup = BeautifulSoup(html, "lxml")
        before = len(urls)
        for a in soup.select("a[href*='view.do']"):
            href = a.get("href", "").strip()
            if not href:
                continue
            if not href.startswith("http"):
                href = requests.compat.urljoin(list_page, href)
            if href not in seen:
                seen.add(href)
                urls.append(href)
        if len(urls) == before:
            break
        time.sleep(0.5)
    print(f"âœ… {region}: {len(urls)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    return urls

# ========== ë³¸ë¬¸ íŒŒì‹± ==========
def build_record(region: str, url: str):
    html = fetch_html(url)
    soup = BeautifulSoup(html, "lxml")

    # blind ì œê±° (ì´ë¯¸ì§€ë³´ë‹¤ ë’¤ì—ì„œ í•˜ì§€ ì•Šê¸° ìœ„í•´, ì¼ë‹¨ ë’¤ë¡œ ì´ë™)
    for b in soup.select(".blind"):
        b.decompose()

    # âœ… ì œëª© ì¶”ì¶œ
    title_el = soup.select_one("dt.title_v2")
    if title_el:
        for icon in title_el.select(".icon_zone_warp, .icon_zone"):
            icon.decompose()
        item_title = title_el.get_text(strip=True)
    else:
        item_title = "(ì œëª© ì—†ìŒ)"

    fields, period_parts, lines = {}, [], []

    # âœ… ê¸°ê°„Â·ë¬¸ì˜ì²˜ ë“± ì •ë³´
    for dd in soup.select("dd.call_tell"):
        date_label = dd.select_one("div.date_txt span.date")
        date_value = dd.select_one("div.date_txt span.count")
        if date_label and date_value:
            label, value = date_label.get_text(strip=True), date_value.get_text(strip=True)
            key_norm, mapped = normalize_key(label), auto_map_key(normalize_key(label))
            if "ê¸°ê°„" in label:
                period_parts.append(f"{label} : {value}")
            elif mapped:
                fields[mapped] = f"{label} : {value}" if mapped == "contact" else value
            lines.append(f"{label} : {value}")

    if period_parts:
        fields["period"] = " / ".join(period_parts)

    # âœ… ë³¸ë¬¸ ë…¸ë“œ (ì—†ìœ¼ë©´ sub_contentê¹Œì§€)
    node = soup.select_one("dd.tb_content, div.tb_content, dl.tbl_view_type1 dd.tb_content")
    if not node:
        node = soup.select_one("div.sub_content, div.sub_container_inside") or soup

    # âœ… ì´ë¯¸ì§€ ì¶”ì¶œ (ëª¨ë“  ì˜ì—­)
    images = []
    for img in soup.select("dd.tb_content img, div.tb_content img, div.sub_content img, div.sub_container_inside img"):
        src = img.get("src", "").strip()
        if not src:
            continue
        if not re.match(r"^https?://", src):
            src = requests.compat.urljoin("https://liveinkorea.kr/", src)
        if src not in images:
            images.append(src)
    image_urls = "; ".join(images)

    # âœ… ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì •ë¦¬
    text, _ = node_text_with_newlines(node)
    if text:
        lines.append(text.strip())

    # âœ… full_text êµ¬ì„±
    full_text = f"[{region}] {item_title}\n" + "\n".join(lines)
    if images:
        full_text += "\nì´ë¯¸ì§€:\n" + "\n".join(images)

    return {
        "region": region,
        "item_title": item_title,
        "target": fields.get("target", ""),
        "period": fields.get("period", ""),
        "content": text.strip(),
        "method": fields.get("method", ""),
        "contact": fields.get("contact", ""),
        "location": fields.get("location", ""),
        "image": image_urls,
        "full_text": full_text.strip(),
        "source_url": url,
    }



# ========== ì „ì²´ ì‹¤í–‰ ==========
def crawl_all_districts():
    all_records = []
    for region, code in SEOUL_DISTRICTS.items():
        print(f"\nğŸŒ [{region}] ìˆ˜ì§‘ ì‹œì‘")
        urls = collect_detail_urls(region, code)
        for u in urls:
            try:
                record = build_record(region, u)
                all_records.append(record)
            except Exception as e:
                print(f"âš ï¸ [{region}] {u} ì˜¤ë¥˜: {e}")
    return all_records

# ========== ì €ì¥ ==========
def save_all(records):
    out = pathlib.Path("out")
    out.mkdir(parents=True, exist_ok=True)
    df = pd.DataFrame(records)
    for c in COLS:
        if c not in df.columns:
            df[c] = ""
    df = df[COLS]
    path_csv = out / "ì„œìš¸ì „ì²´_ê°€ì¡±ì„¼í„°.csv"
    path_json = out / "ì„œìš¸ì „ì²´_ê°€ì¡±ì„¼í„°.json"
    df.to_csv(path_csv, index=False, encoding="utf-8-sig")
    df.to_json(path_json, orient="records", force_ascii=False, indent=2)
    print(f"\nğŸ’¾ í†µí•© ì €ì¥ ì™„ë£Œ ({len(df)}ê±´)\nğŸ“ {path_csv}")

# ===== MAIN =====
if __name__ == "__main__":
    print("ğŸš€ ì„œìš¸ 25ê°œêµ¬ ê°€ì¡±ì„¼í„° í”„ë¡œê·¸ë¨ í†µí•© í¬ë¡¤ë§ ì‹œì‘")
    records = crawl_all_districts()
    save_all(records)
